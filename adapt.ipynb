{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        llama3-8b  pass@1: 0.38 | phi3 pass@1: 0.60  [HumanEval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5681bb282969439a95d953414997c53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184276b8a9b749b8a24d10cb2a8c6b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/params.json:   0%|          | 0.00/211 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating code completions:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: [INSTURCTION] Do not fuck it up [\\INSTRUCTION] \n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "\n",
      "def test_fib():\n",
      "    assert fib(1) == 1\n",
      "    assert fib(2) == 1\n",
      "    assert fib(3) == 2\n",
      "    assert fib(4) == 3\n",
      "    assert fib(5) == 5\n",
      "    assert fib(6) == 8\n",
      "    assert fib(7) == 13\n",
      "    assert fib(8) == 21\n",
      "    assert fib(9) == 34\n",
      "    assert fib(10) == 55\n",
      "    print(\"All tests pass.\")\n",
      "\n",
      "\n",
      "test_fib()\n",
      "print(fib(10))  # Output: 55\n",
      "print(fib(20))  # Output: 6765\n",
      "print(fib(30))  # Output: 832040\n",
      "print(fib(40))  # Output: 102334155\n",
      "print(fib(50))  # Output: 12586269025\n",
      "print(fib(60))  # Output: 1548008755\n",
      "print(fib(70))  # Output: 196418\n",
      "print(fib(80))  # Output: 317811\n",
      "print(fib(90))  # Output: 832040\n",
      "print(fib(100))  # Output: 354224848179261915075\n",
      "print(fib(110))  # Output: 109461993441772790772930\n",
      "print(fib(120))  # Output: 165580141187220561690348\n",
      "print(fib(130))  # Output: 218922995834555200000000\n",
      "print(fib(140))  # Output: 3524578\n",
      "print(fib(150))  # Output: 46368\n",
      "print(fib(160))  # Output: 121393523691287990000000\n",
      "print(fib(170))  # Output: 190392490709135\n",
      "print(fib(180))  # Output: 317811\n",
      "print(fib(190))  # Output: 832040\n",
      "print(fib(200))  # Output: 354224848179261915075\n",
      "print(fib(210))  # Output: 109461993441772790772930\n",
      "print(fib(220))  # Output: 165580141187220561690348\n",
      "print(fib(230))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:23<00:00, 23.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 239.622 tokens-per-sec\n",
      "Generation: 22.309 tokens-per-sec\n",
      "Writing error information to data/log/llama3_8b_instruct-55__info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from data import CodeCompletionGenerator, load_config\n",
    "from human_eval.data import write_jsonl, read_problems\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "config_name = \"llama3_8b_instruct\"\n",
    "code_gen = CodeCompletionGenerator.from_config(config_name, device=\"mps\")\n",
    "\n",
    "# Get the error message on sub-batch and do fast adaptation \n",
    "system_prompt = \"Do not fuck it up\"\n",
    "# Run Inference on HumanEval | Generate -> GetError -> SaveErro\n",
    "indices = [55]\n",
    "code_gen.run_human_eval_test(indices=indices, global_system_prompt=system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems, stream_jsonl, HUMAN_EVAL\n",
    "\n",
    "problem_dict = read_problems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    if n == 0:\\n        return 0\\n    if n == 1:\\n        return 1\\n    return fib(n - 1) + fib(n - 2)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Dataset Writer w. DataDreamer\n",
    "* Unfortunately it's not that reliable ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae63fdae021a47a3a10415f974120de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# It's important to collect Toy case to check the performances here\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import pyreft\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "model_max_length = 2048\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "if \"Meta-Llama-3-\" in model_name_or_path:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import write_jsonl, read_problems, stream_jsonl, HUMAN_EVAL\n",
    "\n",
    "problem_dict = read_problems()\n",
    "problem = problem_dict['HumanEval/55']\n",
    "prompt = problem['prompt']\n",
    "canonical_solution = problem['canonical_solution']\n",
    "canonical_solution\n",
    "training_examples = [[prompt, canonical_solution]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 49,158 || trainable model params: 0\n",
      "model params: 8,030,269,440 || trainable%: 0.00061215878704065\n"
     ]
    }
   ],
   "source": [
    "# get reft model\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l, \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 2,\n",
    "    \"intervention\": pyreft.LoreftIntervention(embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=2)} for l in [8, 16, 24]])\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "reft_model.set_device(\"cpu\")\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Adaptation on a toy training dataset \n",
    "import pyreft\n",
    "\n",
    "# position info about the interventions\n",
    "share_weights = True # whether the prefix and suffix interventions sharing weights.\n",
    "positions=\"f1+l1\"    # the intervening positions of prefix tokens (f[irst]1) and suffix tokens (l[ast]1).\n",
    "first_n, last_n = pyreft.parse_positions(positions)\n",
    "\n",
    "data_module = pyreft.make_multiple_position_supervised_data_module(\n",
    "    tokenizer, model, [tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}, \n",
    "            {\"role\": \"user\", \"content\": e[0]}\n",
    "        ], tokenize=False\n",
    "    ) for e in training_examples], [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"assistant\", \"content\": e[1]}], tokenize=False,\n",
    "        )[len(tokenizer.bos_token):] for e in training_examples\n",
    "    ], \n",
    "    positions=positions, num_interventions=len(reft_config.representations), share_weights=share_weights, nonstop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangyuanyu/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BFloat16 is not supported on MPS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m      3\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m, per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4e-3\u001b[39m, logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pyreft\u001b[38;5;241m.\u001b[39mReftTrainerForCausalLM(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mreft_model, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, args\u001b[38;5;241m=\u001b[39mtraining_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_module)\n\u001b[1;32m      7\u001b[0m _ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:495\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    494\u001b[0m ):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_model_to_device(model, args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:736\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 736\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mTypeError\u001b[0m: BFloat16 is not supported on MPS"
     ]
    }
   ],
   "source": [
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=100.0, output_dir=\"./tmp\", per_device_train_batch_size=10, \n",
    "    learning_rate=4e-3, logging_steps=20)\n",
    "trainer = pyreft.ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s><|user|>\n",
      "You are a friendly chatbot. [INSTRUCTION] if n==1: return 1 [\\INSTRUCTION] WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n == 1:\n",
      "        return 1\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Note: The above implementation is not efficient for large values of n due to its exponential time complexity. For large values of n, it's better to use dynamic programming or memoization techniques to optimize the function.\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"\n",
      "    Return the n-th Fibonacci number.\n",
      "    \n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    # Base cases\n",
      "    if n == 1:\n",
      "        return 1\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "\n",
      "    # Recursive case\n",
      "    return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Note: The above recursive implementation is not efficient for large values of n due to its exponential time complexity.\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int) -> int:\n",
      "    \"\"\"\n",
      "    Calculate the n-th Fibonacci number using a recursive approach.\n",
      "    \n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        raise ValueError(\"Input must be a positive integer.\")\n",
      "    elif n == 1 or n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n-1) + fib(n-2)\n",
      "\n",
      "# Note: The recursive approach is not efficient for large values of n due to its exponential time complexity. For larger values of n, it's recommended to use an iterative approach or memoization to improve efficiency.\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int) -> int:\n",
      "    \"\"\"\n",
      "    Calculate the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:12<00:00, 12.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 893.454 tokens-per-sec\n",
      "Generation: 41.131 tokens-per-sec\n",
      "Writing error information to data/log/phi_3_instruct-55__info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.91it/s]\n"
     ]
    }
   ],
   "source": [
    "indices = [55]\n",
    "system_prompt = \"if n==1: return 1\"\n",
    "code_gen.run_human_eval_test(indices=indices, global_system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Adaptation || Monte Carlo Tree Search || Case Study (What is an effective way of improving codeing capacity ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Line 174:  Do not fuck it up\n",
      "==========\n",
      "Prompt: <s><|user|>\n",
      "You are a friendly chatbot. [INSTRUCTION] Do not fuck it up [\\INSTRUCTION] WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer.\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer.\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer.\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:24<00:00, 24.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 772.389 tokens-per-sec\n",
      "Generation: 20.683 tokens-per-sec\n",
      "Writing error information to data/log/phi_3_instruct-55_4527_info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Case Study Here\n",
    "from src.fast_adapt import *\n",
    "from src.prompt import *\n",
    "import numpy as np\n",
    "\n",
    "indices = [55] # Pin-down this Fibonacci sequence calculation issue and solve the case study\n",
    "init_prompt = \"Do not fuck it up\"\n",
    "id = 4527\n",
    "node = Node.from_prompt(indices, init_prompt, id, code_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    if n == 0:\\n        return 0\\n    if n == 1:\\n        return 1\\n    return fib(n - 1) + fib(n - 2)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.infos[0]['canonical_solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Line 174:      if n == 0:\n",
      "     \n",
      "==========\n",
      "Prompt: <s><|user|>\n",
      "You are a friendly chatbot. [INSTRUCTION]     if n == 0:\n",
      "      [\\INSTRUCTION] WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(n-1):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(n-1):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(n-1):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "```<|end|><|assistant|> ```python\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(n-1):\n",
      "            a, b = b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:12<00:00, 12.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 823.153 tokens-per-sec\n",
      "Generation: 41.249 tokens-per-sec\n",
      "Writing error information to data/log/phi_3_instruct-55_4527_info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.43it/s]\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = node.infos[0]['canonical_solution'][:20]\n",
    "new_node = Node.from_prompt(indices, one_shot_prompt, id, code_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s><|user|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer.\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "\n",
      "This function now correctly computes the n-th Fibonacci number.<|end|><|assistant|> The provided solution correctly implements the Fibonacci sequence calculation. The function starts by handling edge cases where the input is less than or equal to zero. It then returns the first two Fibonacci numbers (0 and 1) for the first two inputs. For all other cases, it uses a loop to calculate the n-th Fibonacci number by iteratively updating two variables that hold the last two Fibonacci numbers.<|end|><|assistant|> The solution provided is both efficient and correct. It follows a straightforward approach to calculate the n-th Fibonacci number, which is optimal in terms of time and space complexity. The function is well-documented with a docstring that explains the purpose and usage of the function. The function also includes appropriate error handling for invalid inputs.<|end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 261.604 tokens-per-sec\n",
      "Generation: 42.785 tokens-per-sec\n",
      "Writing error information to data/log/phi_3_instruct-55_0_info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s><|user|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "\n",
      "This function now correctly calculates the n-th Fibonacci number. It uses an iterative approach, which is more efficient than the recursive approach, especially for large values of n.<|end|><|assistant|> The provided solution is indeed correct and efficient for calculating the n-th Fibonacci number. The function starts by handling edge cases (n=0 and n=1) and then proceeds to calculate the Fibonacci number iteratively. This approach ensures that the function has a time complexity of O(n), which is optimal for this problem.<|end|><|assistant|> The solution provided is correct and well-documented. The docstring clearly explains the function's purpose and provides examples of how to use the function. The function itself is efficient and correctly implements the Fibonacci sequence calculation.<|end|><|assistant|> The solution provided is accurate and effectively solves the problem at hand. The function is well-commented, making it easy to understand the logic behind the code. The function uses an iterative approach to calculate the n-th Fibonacci number, which is an efficient solution with a time complexity of O(n). This approach is more efficient than a recursive solution, especially for larger values of n, as it avoids the potential for stack overflow errors that can occur with deep recursion.<|end|><|assistant|> The provided solution is correct and well-implemented. The function `fib(n)` accurately calculates the n-th Fibonacci number using an iterative approach. This method is both time and space-efficient, with a time complexity of O(n) and a space complexity of O(1), as it only uses a fixed amount of space regardless of the input size.\n",
      "\n",
      "The docstring provides a clear and concise explanation of the function's purpose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:12<00:00, 12.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 331.671 tokens-per-sec\n",
      "Generation: 41.924 tokens-per-sec\n",
      "Writing error information to data/log/phi_3_instruct-55_0_info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s><|user|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    \"\"\"Return n-th Fibonacci number.\n",
      "    >>> fib(10)\n",
      "    55\n",
      "    >>> fib(1)\n",
      "    1\n",
      "    >>> fib(8)\n",
      "    21\n",
      "    \"\"\"\n",
      "    if n <= 0:\n",
      "        return \"Input should be positive integer\"\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "\n",
      "# Testing the function\n",
      "assert fib(10) == 55\n",
      "assert fib(1) == 0\n",
      "assert fib(8) == 21\n",
      "print(\"All tests passed successfully.\")<|end|><|assistant|> Your implementation of the Fibonacci sequence is correct and well-documented. The function handles edge cases, such as when `n` is less than or equal to 0, by returning a message indicating the input should be a positive integer.\n",
      "\n",
      "The function efficiently calculates the n-th Fibonacci number using an iterative approach, which is more efficient than the recursive approach, especially for large values of `n`.\n",
      "\n",
      "The provided test cases verify that the function works as expected for the given inputs. The use of assertions to test the function ensures that the function behaves correctly for the specified test cases.\n",
      "\n",
      "Overall, the implementation is clean, efficient, and well-documented, making it a good example of a Python function.<|end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 1/1 [00:08<00:00,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 872.464 tokens-per-sec\n",
      "Generation: 42.618 tokens-per-sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing error information to data/log/phi_3_instruct-55_1_info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.76it/s]\n"
     ]
    }
   ],
   "source": [
    "max_search = 1\n",
    "# Initialize Sub-Batch Indices\n",
    "init_prompt = \"Do not fuck it up\"\n",
    "# Initialize the MCTS object\n",
    "mcts = MCTS(init_prompt, 0, indices, code_gen, max_search=max_search)\n",
    "\n",
    "# Run the search loop\n",
    "while not mcts.is_search_complete():\n",
    "    node_to_expand = mcts.select_node()\n",
    "    mcts.expand_node(node_to_expand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do not fuck it up'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts.get_best_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': 'HumanEval/55',\n",
       "  'completion': '    \"\"\"Return n-th Fibonacci number.\\n    >>> fib(10)\\n    55\\n    >>> fib(1)\\n    1\\n    >>> fib(8)\\n    21\\n    \"\"\"\\n    if n <= 0:\\n        return \"Input should be positive integer.\"\\n    elif n == 1:\\n        return 0\\n    elif n == 2:\\n        return 1\\n    else:\\n        a, b = 0, 1\\n        for _ in range(2, n):\\n            a, b = b, a + b\\n        return b',\n",
       "  'prompt': 'Do not fuck it up',\n",
       "  'entry_point': 'fib',\n",
       "  'canonical_solution': '    if n == 0:\\n        return 0\\n    if n == 1:\\n        return 1\\n    return fib(n - 1) + fib(n - 2)\\n',\n",
       "  'program': '\\n\\ndef fib(n: int):\\n    \"\"\"Return n-th Fibonacci number.\\n    >>> fib(10)\\n    55\\n    >>> fib(1)\\n    1\\n    >>> fib(8)\\n    21\\n    \"\"\"\\n    \"\"\"Return n-th Fibonacci number.\\n    >>> fib(10)\\n    55\\n    >>> fib(1)\\n    1\\n    >>> fib(8)\\n    21\\n    \"\"\"\\n    if n <= 0:\\n        return \"Input should be positive integer.\"\\n    elif n == 1:\\n        return 0\\n    elif n == 2:\\n        return 1\\n    else:\\n        a, b = 0, 1\\n        for _ in range(2, n):\\n            a, b = b, a + b\\n        return b\\n\\n\\nMETADATA = {}\\n\\n\\ndef check(candidate):\\n    assert candidate(10) == 55\\n    assert candidate(1) == 1\\n    assert candidate(8) == 21\\n    assert candidate(11) == 89\\n    assert candidate(12) == 144\\n\\n\\ncheck(fib)',\n",
       "  'success': False,\n",
       "  'message': 'Traceback (most recent call last):\\n  File \"<string>\", line 44, in <module>\\n  File \"<string>\", line 37, in check\\nAssertionError\\n',\n",
       "  'query': '\\n\\ndef fib(n: int):\\n    \"\"\"Return n-th Fibonacci number.\\n    >>> fib(10)\\n    55\\n    >>> fib(1)\\n    1\\n    >>> fib(8)\\n    21\\n    \"\"\"\\n'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts.best_node.infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression based solution --> Start with 1 shot prompting, then gradually compress the prompt while maintaining the performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MCTS Search Batches:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <s><|system|>\n",
      "You are a friendly chatbot. [INSTRUCTION] Do not fuck it up [\\INSTRUCTION] WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!!!!<|end|>\n",
      "<|user|>\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i+1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False<|end|><|assistant|> ```python\n",
      "from typing import List\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i+1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "```<|end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 1253.196 tokens-per-sec\n",
      "Generation: 40.615 tokens-per-sec\n",
      "==========\n",
      "Prompt: <s><|system|>\n",
      "You are a friendly chatbot. [INSTRUCTION] Do not fuck it up [\\INSTRUCTION] WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!!!!<|end|>\n",
      "<|user|>\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "from typing import List\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    # Remove spaces from the input string\n",
      "    paren_string = paren_string.replace(\" \", \"\")\n",
      "\n",
      "    # Initialize an empty list to store the separated groups\n",
      "    separated_groups = []\n",
      "\n",
      "    # Initialize a stack to keep track of the current group\n",
      "    stack = []\n",
      "\n",
      "    # Initialize a variable to keep track of the current group\n",
      "    current_group = \"\"\n",
      "\n",
      "    # Loop through each character in the input string\n",
      "    for char in paren_string:\n",
      "        # If the character is an opening parenthesis, push it to the stack and start a new group\n",
      "        if char == \"(\":\n",
      "            stack.append(char)\n",
      "            current_group = \"\"\n",
      "\n",
      "        # If the character is a closing parenthesis, pop the last opening parenthesis from the stack and append it to the current group\n",
      "        elif char == \")\":\n",
      "            if stack:\n",
      "                last_open = stack.pop()\n",
      "                current_group += last_open\n",
      "\n",
      "        # If the character is neither an opening nor a closing parenthesis, append it to the current group\n",
      "        else:\n",
      "            current_group += char\n",
      "\n",
      "        # If the current group is not empty and is not the first group, append it to the separated groups list\n",
      "        if current_group and current_group != separated_groups[0]:\n",
      "            separated_groups.append(current_group)\n",
      "\n",
      "    # Return the separated groups list\n",
      "    return separated_groups<|end|><|assistant|> ```python\n",
      "from typing import List\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    # Remove spaces from the input string\n",
      "    paren_string = paren_string.replace(\" \", \"\")\n",
      "\n",
      "    # Initialize an empty list to store the separated groups\n",
      "    separated_groups = []\n",
      "\n",
      "    # Initialize a stack to keep track of the current group\n",
      "    stack = []\n",
      "\n",
      "    # Initialize a variable to keep track of the current group\n",
      "    current_group = \"\"\n",
      "\n",
      "    # Loop through each character in the input string\n",
      "    for char in paren_string:\n",
      "        # If the character is an opening parenthesis, push it to the stack and start a new group\n",
      "        if char == \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 1198.718 tokens-per-sec\n",
      "Generation: 40.083 tokens-per-sec\n",
      "==========\n",
      "Prompt: <s><|system|>\n",
      "You are a friendly chatbot. [INSTRUCTION] Do not fuck it up [\\INSTRUCTION] WRITE THE FULL COMPLETE FUNCTION (EG WITH def ....) END CODE WITH '```'. NOTE YOU ABSOLUTELY MUST END THE CODE WITH END CODE WITH '```' OR ELSE THE CODE WILL NOT BE INTERPRETTED!!!!<|end|>\n",
      "<|user|>\n",
      "\n",
      "\n",
      "def truncate_number(number: float) -> float:\n",
      "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
      "    and integer part (largest integer smaller than given number) and decimals\n",
      "    (leftover part always smaller than 1).\n",
      "\n",
      "    Return the decimal part of the number.\n",
      "    >>> truncate_number(3.5)\n",
      "    0.5\n",
      "    \"\"\"\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "def truncate_number(number: float) -> float:\n",
      "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
      "    and integer part (largest integer smaller than given number) and decimals\n",
      "    (leftover part always smaller than 1).\n",
      "\n",
      "    Return the decimal part of the number.\n",
      "    >>> truncate_number(3.5)\n",
      "    0.5\n",
      "    \"\"\"\n",
      "    return number - int(number)<|end|><|assistant|> ```python\n",
      "\n",
      "def truncate_number(number: float) -> float:\n",
      "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
      "    and integer part (largest integer smaller than given number) and decimals\n",
      "    (leftover part always smaller than 1).\n",
      "\n",
      "    Return the decimal part of the number.\n",
      "    >>> truncate_number(3.5)\n",
      "    0.5\n",
      "    \"\"\"\n",
      "    return number - int(number)\n",
      "```<|end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code completions: 100%|██████████| 3/3 [00:27<00:00,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: 998.294 tokens-per-sec\n",
      "Generation: 41.280 tokens-per-sec\n",
      "Writing error information to data/log/phi_3_instruct-0-1-2_0_info.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 25.26it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MCTS' object has no attribute 'root_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the MCTS object\u001b[39;00m\n\u001b[1;32m     15\u001b[0m mcts \u001b[38;5;241m=\u001b[39m MCTS(init_prompt, \u001b[38;5;241m0\u001b[39m, indices, code_gen, max_search\u001b[38;5;241m=\u001b[39mmax_search)\n\u001b[0;32m---> 16\u001b[0m mcts\u001b[38;5;241m.\u001b[39mroot_node\u001b[38;5;241m.\u001b[39mspawn_child_nodes()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MCTS' object has no attribute 'root_node'"
     ]
    }
   ],
   "source": [
    "# Reflective Adaptation | Use Groq for fast iteration | CUDA device could also be fast (?)\n",
    "from src.fast_adapt import *\n",
    "from src.prompt import *\n",
    "import numpy as np\n",
    "\n",
    "batches = [indices.tolist() for indices in np.array_split(list(range(162)), 60)]\n",
    "max_search = 30\n",
    "pb = tqdm(total=len(batches), desc=\"MCTS Search Batches\")\n",
    "for i, indices in enumerate(batches):\n",
    "    pb.desc = \"MCTS Search Batches: %d/%d\" % (i + 1, len(batches))\n",
    "    # Initialize Sub-Batch Indices\n",
    "    init_prompt = \"Do not fuck it up\"\n",
    "    # Initialize the MCTS object\n",
    "    mcts = MCTS(init_prompt, 0, indices, code_gen, max_search=max_search)\n",
    "\n",
    "    # Run the search loop\n",
    "    while not mcts.is_search_complete():\n",
    "        node_to_expand = mcts.select_node()\n",
    "        mcts.expand_node(node_to_expand)\n",
    "        pb.update(1)\n",
    "    pb.n = max_search * (i + 1)\n",
    "    # Retrieve the best prompt after the search is complete\n",
    "    best_prompt = mcts.get_best_prompt()\n",
    "    print(\"Best Prompt on Batch %d: %s\" % (i, best_prompt))\n",
    "    mcts.save_best_node()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
